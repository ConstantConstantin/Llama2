<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Inference · Llama2.jl</title><meta name="title" content="Inference · Llama2.jl"/><meta property="og:title" content="Inference · Llama2.jl"/><meta property="twitter:title" content="Inference · Llama2.jl"/><meta name="description" content="Documentation for Llama2.jl."/><meta property="og:description" content="Documentation for Llama2.jl."/><meta property="twitter:description" content="Documentation for Llama2.jl."/><meta property="og:url" content="https://ConstantConstantin.github.io/Llama2.jl/inference/"/><meta property="twitter:url" content="https://ConstantConstantin.github.io/Llama2.jl/inference/"/><link rel="canonical" href="https://ConstantConstantin.github.io/Llama2.jl/inference/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">Llama2.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li class="is-active"><a class="tocitem" href>Inference</a><ul class="internal"><li><a class="tocitem" href="#Prequisites"><span>Prequisites</span></a></li><li><a class="tocitem" href="#Inferencing"><span>Inferencing</span></a></li></ul></li><li><a class="tocitem" href="../devs/">Developer&#39;s Corner</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Inference</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Inference</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/ConstantConstantin/Llama2.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/ConstantConstantin/Llama2.jl/blob/main/docs/src/inference.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Inference"><a class="docs-heading-anchor" href="#Inference">Inference</a><a id="Inference-1"></a><a class="docs-heading-anchor-permalink" href="#Inference" title="Permalink"></a></h1><h2 id="Prequisites"><a class="docs-heading-anchor" href="#Prequisites">Prequisites</a><a id="Prequisites-1"></a><a class="docs-heading-anchor-permalink" href="#Prequisites" title="Permalink"></a></h2><p>A model checkpoint is required. You can use your own or e.g. get the example file provided by karpathy:</p><pre><code class="nohighlight hljs">wget https://huggingface.co/karpathy/tinyllamas/resolve/main/stories15M.bin</code></pre><h2 id="Inferencing"><a class="docs-heading-anchor" href="#Inferencing">Inferencing</a><a id="Inferencing-1"></a><a class="docs-heading-anchor-permalink" href="#Inferencing" title="Permalink"></a></h2><p>You can either generate a single text, optionally giving an input prompt, or have an interactive chat.</p><article><details class="docstring" open="true"><summary id="Llama2.talktollm"><a class="docstring-binding" href="#Llama2.talktollm"><code>Llama2.talktollm</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">talktollm(modelpath::String, [prompt::String]; max_tokens::Int, vocabpath::String, verbose::Bool)</code></pre><p>Generate text using a pretrained LLama2 transformer model. Return that text as a <code>String</code>. Load the model from <code>modelpath</code> and the corresponding tokenizer from <code>vocabpath</code> (which defaults to <code>&quot;data/tokenizer.bin&quot;</code>). Take an initial <code>prompt</code> <code>String</code> to start the text generation and generate up to <code>max_tokens</code> tokens. If <code>verbose</code>, print the text during generation.</p><pre><code class="language-julia hljs">julia&gt; print(talktollm(&quot;/PATH/TO/YOUR/MODEL.bin&quot;))
 Once upon a time, there was a little girl named Lily. She loved to play outside in the park with her friends. One day, Lily was running and she fell and hit her head on a rock. She got a big ouchie and it started to bleed. 
Lily&#39;s mom took her to the doctor and the doctor said she needed a stitch. Lily was scared, but her mom was very dependable and told her they would be coming back home soon. 
After the doctor fixed Lily&#39;s knee, they went home and Lily&#39;s friends came to play again. But Lily&#39;s mom noticed that she was playing with a ball and some new toys. This made her very happy.

julia&gt; print(talktollm(&quot;/PATH/TO/YOUR/MODEL.bin&quot;, &quot;&quot;What is this?&quot;&quot;))
&quot;What is this?&quot; the woman asked.
The little girl looked at the bookion and said, &quot;This is a book about a princess. Maybe we can use it together.&quot;
They decided to sit down and read the book together. They read about a beautiful garden with lovely flowers. The little girl loved the book very much and said, &quot;I want to be a princess again!&quot;
&quot;Maybe, if you read me another book,&quot; the woman said.
From that day on, they would sit down and read the book every night before bed. They hoped that when they finished reading it, something magical would happen.</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/ConstantConstantin/Llama2.jl/blob/ef20e000b0136901c2b893844461a8380dea9024/src/talk.jl#L1-L24">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Llama2.ChatBot"><a class="docstring-binding" href="#Llama2.ChatBot"><code>Llama2.ChatBot</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">ChatBot(path::String; vocabpath::String)</code></pre><p>Create a <code>ChatBot</code> constructing a <code>Transformer</code> from <code>path</code>.</p><p><code>vocabpath</code> defaults to <code>&quot;data/tokenizer.bin&quot;</code>. The <code>ChatBot</code> struct is used with <a href="#Llama2.chatwithllm"><code>chatwithllm</code></a> for continuous text generation.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/ConstantConstantin/Llama2.jl/blob/ef20e000b0136901c2b893844461a8380dea9024/src/structs.jl#L88-L95">source</a></section></details></article><article><details class="docstring" open="true"><summary id="Llama2.chatwithllm"><a class="docstring-binding" href="#Llama2.chatwithllm"><code>Llama2.chatwithllm</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">chatwithllm(bot::ChatBot, [prompt::String]; max_tokens::Int, verbose::Bool)</code></pre><p>Generate text using a pretrained LLama2 transformer model. Return that text as a <code>String</code>.</p><p>Multiple calls on the same instance of <code>ChatBot</code> respect the previously generated tokens and continue generation from there. Take an initial <code>prompt</code> <code>String</code> to start the text generation and generate up to <code>max_tokens</code> tokens. If <code>verbose</code>, print the text during generation.</p><pre><code class="language-julia hljs">julia&gt; c = ChatBot(&quot;data/stories15M.bin&quot;);

julia&gt; print(chatwithllm(c); max_tokens = 63)
 Once upon a time, there was an old house with an ancient sign inside. The sign was very big and had many words on it. One day, a little girl went to visit the old house. She wanted to see what was inside.
The old house said, &quot;Hello? Can I come in?&quot;

julia&gt; print(chatwithllm(c, &quot;
The little girl said:&quot;; max_tokens = 63))

The little girl said: &quot;Yes please! Can I come in too?&quot;
The old house thought for moments before it said, &quot;Yes. This light is available for you 30 cent a nightmare.&quot;
The little girl was very excited. She said thank you and then, followed her favorite sign

julia&gt; print(chatwithllm(c, &quot;until&quot;; max_tokens = 63))
until she saw there was a beautiful light online.
When the old house passed, the girl happily went inside. It was very old, but it had been there for a long time. The old house was very special, and she thought the light was the prettiest thing ever.</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/ConstantConstantin/Llama2.jl/blob/ef20e000b0136901c2b893844461a8380dea9024/src/talk.jl#L70-L99">source</a></section></details></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Home</a><a class="docs-footer-nextpage" href="../devs/">Developer&#39;s Corner »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Friday 30 January 2026 21:03">Friday 30 January 2026</span>. Using Julia version 1.12.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
