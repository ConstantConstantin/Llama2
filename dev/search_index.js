var documenterSearchIndex = {"docs":
[{"location":"#Llama2","page":"Home","title":"Llama2","text":"Documentation for Llama2.\n\n","category":"section"},{"location":"#Llama2.TokenIndex","page":"Home","title":"Llama2.TokenIndex","text":"TokenIndex(str::String, id::Integer)\n\nCreate a TokenIndex from a string and an integer identifier.\n\nThe byte sequence is converted to String and the ID is converted to Int16.   Throw a DomainError if id â‰¤ 0.\n\nExamples\n\njulia> using Llama2;\n\njulia> TokenIndex(\"Julia\", 1)\nTokenIndex(\"Julia\", 1)\n\njulia> TokenIndex(\"Julia\", -1)\nERROR: DomainError with Token index must be > 0.\n[...]\n\nDeveloper Notes\n\nThis is an internal struct.\n\n\n\n\n\n","category":"type"},{"location":"#Llama2.Tokenizer","page":"Home","title":"Llama2.Tokenizer","text":"Tokenizer\n\nConstruct a tokenizer storing vocabulary entries, scores, and byte-piece mappings.\n\nConstructors\n\nTokenizer(vocab, vocab_scores, sorted_vocab, vocab_size, max_token_length, byte_pieces)   Construct a tokenizer directly from the provided fields.   Validate that max_token_length > 0 and that byte_pieces has length 256.\nTokenizer(path::String, vocab_size::Integer)   Load a tokenizer from a binary file.\n\nFields\n\nvocab: Token string sequences.  \nvocab_scores: Scores for each token.  \nsorted_vocab: Sorted token indices.  \nvocab_size: Number of vocabulary entries.  \nmax_token_length: Maximum token length in bytes.  \nbyte_pieces: Byte mapping (length 256).\n\n\n\n\n\n","category":"type"},{"location":"#Llama2.compare_tokens-Tuple{TokenIndex, TokenIndex}","page":"Home","title":"Llama2.compare_tokens","text":"compare_tokens(first_token::TokenIndex, second_token::TokenIndex) -> Bool\n\nCompare two TokenIndex objects by their string values. It returns true if the first token's string is lexicographically less than the second's, and false otherwise.\n\nExamples\n\njulia> using Llama2;\n\njulia> compare_tokens(TokenIndex(\"A\", 1), TokenIndex(\"B\", 2))\ntrue\n\njulia> compare_tokens(TokenIndex(\"B\", 1), TokenIndex(\"A\", 2))\nfalse\n\n\n\n\n\n","category":"method"},{"location":"#Llama2.encode-Tuple{Tokenizer, String}","page":"Home","title":"Llama2.encode","text":"encode\n\nConverts a string text into a sequence of token IDs using a Tokenizer. First ensure the tokenizer's vocabulary is sorted, then encode each character into its corresponding ID. After that, iteratively merge token pairs with the highest scores to form longer tokens until no more merges are possible. Return the final token ID sequence.\n\n\n\n\n\n","category":"method"},{"location":"#Llama2.str_lookup-Tuple{String, Vector{TokenIndex}}","page":"Home","title":"Llama2.str_lookup","text":"str_lookup(str::String, sorted_vocab::Vector{TokenIndex}) -> Int16\n\nSearch for a given string str within a sorted vocabulary sorted_vocab of TokenIndex objects. If the string is found, it returns the corresponding token ID; otherwise, it returns -1. It uses a binary search for efficient lookup.\n\nExamples\n\njulia> using Llama2;\n\njulia> str_lookup(\"aa\", [TokenIndex(\"aa\", 1), TokenIndex(\"bb\", 2)])\n1\n\njulia> str_lookup(\"ba\", [TokenIndex(\"aa\", 1), TokenIndex(\"bb\", 2)])\n-1\n\n\n\n\n\n","category":"method"}]
}
