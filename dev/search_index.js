var documenterSearchIndex = {"docs":
[{"location":"devs/#Developer's-Corner","page":"Developer's Corner","title":"Developer's Corner","text":"You want to understand how this package works or modify the code you are running? Here the necessary tools are provided and explained.","category":"section"},{"location":"devs/#Tokenizer","page":"Developer's Corner","title":"Tokenizer","text":"","category":"section"},{"location":"devs/#Transformer","page":"Developer's Corner","title":"Transformer","text":"","category":"section"},{"location":"devs/#forward!","page":"Developer's Corner","title":"forward!","text":"","category":"section"},{"location":"devs/#Sampler","page":"Developer's Corner","title":"Sampler","text":"","category":"section"},{"location":"devs/#Llama2.Tokenizer","page":"Developer's Corner","title":"Llama2.Tokenizer","text":"Tokenizer\n\nConstruct a tokenizer storing vocabulary entries, scores, and byte-piece mappings.\n\nConstructors\n\nTokenizer(vocab, vocab_scores, sorted_vocab, vocab_size, max_token_length, byte_pieces)   Construct a tokenizer directly from the provided fields.   Validate that max_token_length > 0 and that byte_pieces has length 256.\nTokenizer(path::String, vocab_size::Integer)   Load a tokenizer from a binary file.\n\nFields\n\nvocab: Token string sequences.  \nvocab_scores: Scores for each token.  \nsorted_vocab: Sorted token indices.  \nvocab_size: Number of vocabulary entries.  \nmax_token_length: Maximum token length in bytes.  \nbyte_pieces: Byte mapping (length 256).\n\n\n\nTokenizer(path::String, vocab_size::Integer) -> Tokenizer\n\nLoad a tokenizer from a binary file.\n\nRead vocabulary, token scores, and metadata from the binary file at path. The file format expects:\n\nInt32: maximum token length\nFor each of vocab_size tokens:\nFloat32: token score\nInt32: string length (n)\nn bytes: token string (UTF-8)\n\nArguments\n\npath::String: Path to the tokenizer binary file.\nvocab_size::Integer: Expected number of vocabulary entries.\n\nReturns\n\nTokenizer: A tokenizer ready for encoding/decoding text.\n\n\n\n\n\n","category":"type"},{"location":"devs/#Llama2.TokenIndex","page":"Developer's Corner","title":"Llama2.TokenIndex","text":"TokenIndex(str::String, id::Integer)\n\nA vocabulary token with its string representation and numeric identifier.\n\nStores a token string and its associated ID for efficient sorting and lookup in the tokenizer vocabulary.\n\nThrow a DomainError if id ≤ 0.\n\nArguments\n\nstr::String: Token string (e.g., \"Julia\").\nid::Integer: Token identifier (converted to Int16, must be ≥ 0).\n\nExamples\n\njulia> Llama2.TokenIndex(\"Julia\", 1)\nLlama2.TokenIndex(\"Julia\", 1)\n\njulia> Llama2.TokenIndex(\"Julia\", -1)\nERROR: DomainError with Token index must be > 0.\n[...]\n\nDeveloper Notes\n\nThis is an internal struct.\n\n\n\n\n\n","category":"type"},{"location":"devs/#Base.isless-Tuple{Llama2.TokenIndex, Llama2.TokenIndex}","page":"Developer's Corner","title":"Base.isless","text":"Base.isless(first_token::TokenIndex, second_token::TokenIndex) -> Bool\n\nCompare two tokens lexicographically by their string values.\n\nReturn true if first_token.str < second_token.str in lexicographic order. Intended for use as the lt argument to sorting functions.\n\nExamples\n\njulia> isless(Llama2.TokenIndex(\"A\", 1), Llama2.TokenIndex(\"B\", 2))\ntrue\n\njulia> isless(Llama2.TokenIndex(\"B\", 1), Llama2.TokenIndex(\"A\", 2))\nfalse\n\n\n\n\n\n","category":"method"},{"location":"devs/#Llama2.str_lookup","page":"Developer's Corner","title":"Llama2.str_lookup","text":"str_lookup(str::String, sorted_vocab::Vector{TokenIndex}) -> Int16\n\nSearch for str within a sorted vocabulary sorted_vocab. If a match is found, it returns the corresponding token ID; otherwise, it returns -1. It uses a binary search for efficient lookup.\n\nArguments\n\nstr::String: Token string to search for.\nsorted_vocab::Vector{TokenIndex}: Vocabulary sorted lexicographically by string.\n\nReturns\n\nInt16: Token ID if found, -1 otherwise.\n\nExamples\n\njulia> Llama2.str_lookup(\"aa\", [Llama2.TokenIndex(\"aa\", 1), Llama2.TokenIndex(\"bb\", 2)])\n1\n\njulia> Llama2.str_lookup(\"ba\", [Llama2.TokenIndex(\"aa\", 1), Llama2.TokenIndex(\"bb\", 2)])\n-1\n\n\n\n\n\n","category":"function"},{"location":"devs/#Llama2.encode","page":"Developer's Corner","title":"Llama2.encode","text":"encode(tokenizer::Tokenizer, text::String) -> Vector{Integer}\n\nConverts text into a sequence of token IDs using tokenizer. First ensure the tokenizer's vocabulary is sorted, then encode each character into its corresponding ID. After that, iteratively merge token pairs with the highest scores to form longer tokens until no more merges are possible. Return the final token ID sequence.\n\nArguments\n\ntokenizer::Tokenizer: The tokenizer with vocabulary and merge scores.\ntext::String: Input text to encode.\n\nReturns\n\nVector{Integer}: Sequence of token IDs representing the encoded text.\n\n\n\n\n\n","category":"function"},{"location":"devs/#Llama2.Transformer","page":"Developer's Corner","title":"Llama2.Transformer","text":"Transformer(config::Config, weights::TransformerWeights)\n\nCreate a Transformer with data from config and weights. The RunState containers are initialized empty and just are assigned the corresponding dimensions.\n\nDeveloper Notes\n\nThis is an internal struct.\n\n\n\n\n\n","category":"type"},{"location":"devs/#Llama2.Config","page":"Developer's Corner","title":"Llama2.Config","text":"Config\n\nCreate a Config containing 7 Int32. These describe meta-data to read values from an input file.\n\nDeveloper Notes\n\nThis is an internal struct.\n\n\n\n\n\n","category":"type"},{"location":"devs/#Llama2.TransformerWeights","page":"Developer's Corner","title":"Llama2.TransformerWeights","text":"TransformerWeights\n\nCreate a TransformerWeights containing several Float32 containers. These describe actual weight data that is loaded from an input file.\n\nDeveloper Notes\n\nThis is an internal struct.\n\n\n\n\n\n","category":"type"},{"location":"devs/#Llama2.RunState","page":"Developer's Corner","title":"Llama2.RunState","text":"RunState\n\nCreate a RunState containing several Float32 containers. These reflect the state of the Transformer at run-time.\n\nDeveloper Notes\n\nThis is an internal struct.\n\n\n\n\n\n","category":"type"},{"location":"devs/#Llama2.forward!","page":"Developer's Corner","title":"Llama2.forward!","text":"forward!(transformer::Transformer, token::Int32, pos::Int32)\n\nPerform a single forward pass through the transformer.\n\nCompute logits for the next token prediction given the current token at position pos in the sequence. Updates the internal KV-cache in transformer.state with keys and values from this forward pass.\n\nArguments\n\ntransformer::Transformer: The model (modified in-place via KV-cache updates).\ntoken::Int32: Current input token index (must be in range 1:vocab_size).\npos::Int32: Position in the sequence (1-indexed, must be ≤ seq_len).\n\nReturns\n\nVector{Float32}: Logits over the vocabulary for next token prediction (length = vocab_size).\n\nExamples\n\nmodel = Transformer(\"model.bin\")\ntoken = Int32(1)  # BOS token\npos = Int32(1)\n\nlogits = forward!(model, token, pos)\nnext_token = argmax(logits)\n\n\n\n\n\n","category":"function"},{"location":"devs/#Llama2.rmsnorm","page":"Developer's Corner","title":"Llama2.rmsnorm","text":"rmsnorm(x, w) -> Vector{Float32}\n\nCalculate the rmsnorm of x and w, the scaled product 'λw * x'.\n\nArguments\n\nx::AbstractVector{Float32}: Input vector to normalize.\nw::AbstractVector{Float32}: Scale weights (must have same length as x).\n\nReturns\n\nVector{Float32}: Normalized and scaled output.\n\nExamples\n\njulia>  x = [1.0f0,2,3];\n\njulia>  w = [1.0f0,1,1];\n\njulia> o = Llama2.rmsnorm(x, w) \n3-element Vector{Float32}:\n 0.46290955\n 0.9258191\n 1.3887286\n\n\n\n\n\n","category":"function"},{"location":"devs/#Llama2.softmax!","page":"Developer's Corner","title":"Llama2.softmax!","text":"softmax!(x) -> Vector{Float32}\n\nUpdates the output of an layer 'x' with the softmax! of the input.\n\nTransform logits into a probability distribution by exponentiating and normalizing. Uses the numerically stable formulation: x[i] = exp(x[i] - max(x)) / sum(exp(x .- max(x))).\n\nThe input vector is modified in-place and also returned.\n\nArguments\n\nx::AbstractVector{Float32}: Logits to transform (modified in-place).\n\nReturns\n\nVector{Float32}: The same vector x, now containing probabilities that sum to 1.\n\nExamples\n\njulia> x = [-1.0f0,0,1];\n\njulia> Llama2.softmax!(x);\n\njulia> x\n3-element Vector{Float32}:\n 0.09003057\n 0.24472848\n 0.66524094\n\n\n\n\n\n","category":"function"},{"location":"devs/#Llama2.Sampler","page":"Developer's Corner","title":"Llama2.Sampler","text":"Sampler\n\nStateful sampler for converting model logits into token indices.\n\nEncapsulate sampling configuration (temperature, nucleus sampling) and internal buffers needed for efficient token sampling.\n\nConstructors\n\nSampler(vocab_size::Int32, temperature::Float32, topp::Float32, rng_seed::Int128)   Construct a sampler with automatically allocated internal buffers for nucleus (top-p) sampling.\nSampler(vocab_size::Int32, probindex::Vector{ProbIndex},           temperature::Float32, topp::Float32, rng_state::Int128)   Construct a sampler using a caller-provided probindex workspace buffer. The buffer must have length at least vocab_size.\n\nFields\n\nvocab_size::Int32: Vocabulary size\ntemperature::Float32: Sampling temperature (0 = greedy)\ntopp::Float32: Nucleus sampling threshold\nrng_state::Int128: Random number generator state\n\nSampler is callable and can be applied to a vector of logits to obtain the next token index.\n\n\n\n\n\n","category":"type"},{"location":"devs/#Llama2.ProbIndex","page":"Developer's Corner","title":"Llama2.ProbIndex","text":"ProbIndex\n\nCreate a ProbIndex from an AbstractFloat and an Integer.\n\nprob contains a proberbility and is stored as an Int32 and index is stored as a Float32. Throw a DomainError if index < 0.\n\nExamples\n\njulia> Llama2.ProbIndex(1.0, 2)\nProbIndex(1.0f0, 2)\n\njulia> ProbIndex(1.0, -1)\nERROR: DomainError with Prob index must be > 0.:\n[...]\n\n\n\n\n\n","category":"type"},{"location":"devs/#Base.isless-Tuple{Llama2.ProbIndex, Llama2.ProbIndex}","page":"Developer's Corner","title":"Base.isless","text":"isless(a::ProbIndex, b::ProbIndex) -> Bool\n\nComparison function for ordering ProbIndex values by probability.\n\nReturn true if a.prob < b.prob. Intended for use as the lt argument to sorting routines.\n\n\n\n\n\n","category":"method"},{"location":"devs/#Llama2.sample_mult","page":"Developer's Corner","title":"Llama2.sample_mult","text":"sample_mult(probabilities, coin) -> Int\n\nSample an index from a multinomial distribution.\n\nGiven a vector of normalized probabilities and a uniform random number coin ∈ [0, 1), returns the first index whose cumulative probability exceeds coin.\n\nArguments\n\nprobabilities::Vector{Float32}: Probability mass function (must sum to 1).\ncoin::Float32: Uniform random number in [0, 1).\n\nReturns\n\nIndex of the sampled element.\n\nIf numerical roundoff prevents an early return, the last index is returned.\n\n\n\n\n\n","category":"function"},{"location":"devs/#Llama2.sample_topp","page":"Developer's Corner","title":"Llama2.sample_topp","text":"sample_topp(probabilities, topp, probindex, coin) -> Int\n\nSample an index using nucleus (top-p) sampling.\n\nSelects the smallest set of tokens whose cumulative probability mass exceeds topp, then samples from this restricted distribution using the provided random number.\n\nArguments\n\nprobabilities::Vector{Float32}: Normalized probability distribution.\ntopp::Float32: Cumulative probability threshold (0 < topp < 1).\nprobindex::Vector{ProbIndex}: Preallocated workspace for sorting and indexing candidate tokens.\ncoin::Float32: Uniform random number in [0, 1).\n\nReturns\n\nIndex of the sampled token.\n\nThe probindex buffer is mutated and reused to avoid allocations.\n\n\n\n\n\n","category":"function"},{"location":"#Llama2.jl","page":"Home","title":"Llama2.jl","text":"","category":"section"},{"location":"#What-is-Llama2?","page":"Home","title":"What is Llama2?","text":"LLama2 is a family of pre-trained LLMs by Meta AI. More information can be found at: https://www.llama.com/","category":"section"},{"location":"#What-is-Llama2.jl?","page":"Home","title":"What is Llama2.jl?","text":"Llama2.jl can inference a given model from within julia. For this cause you will have to provide your own model checkpoint. This project follows the procedure outlined by the run.c file from llama2.c.","category":"section"},{"location":"#Getting-started","page":"Home","title":"Getting started","text":"Start julia, activate a desired environment and add the package:\n\n(@v1.11) pkg> activate .\n\n(myLlama2) pkg> add https://github.com/ConstantConstantin/Llama2.jl\n\nIn every subsequent session it can be loaded via:\n\njulia> using Llama2","category":"section"},{"location":"#Example-Usage","page":"Home","title":"Example Usage","text":"julia> print(talktollm(\"/PATH/TO/YOUR/MODEL.bin\", \"In a small village \"))\nIn a small village house, there was a man named Tom. Tom was kind and would always shine his in front of the town. People from the village would come to look at Tom and feel happy.\nOne day, a little girl named Lily came to Tom. She did not have a passport. Tom saw Lily and said, \"Why don't you have a passport, Lily? Hop in and pass me a little in our country!\" Lily smiled and said, \"Yes, I feel comfortable when I am in my own nation!\"\nLily put on her sunglasses and they became good friends. The town was filled with happy puppies who shared their sunglasses with everyone. The people in the town knew that being kind and working together made everything better.\n\n","category":"section"},{"location":"inference/#Inference","page":"Inference","title":"Inference","text":"","category":"section"},{"location":"inference/#Prequisites","page":"Inference","title":"Prequisites","text":"A model checkpoint is required. You can use your own or e.g. get the example file provided by karpathy:\n\nwget https://huggingface.co/karpathy/tinyllamas/resolve/main/stories15M.bin","category":"section"},{"location":"inference/#Inferencing","page":"Inference","title":"Inferencing","text":"You can either generate a single text, optionally giving an input prompt, or have an interactive chat.","category":"section"},{"location":"inference/#Llama2.talktollm","page":"Inference","title":"Llama2.talktollm","text":"talktollm(modelpath::String, [prompt::String]; max_tokens::Int, vocabpath::String, verbose::Bool) -> String\n\nGenerate text using a pretrained LLama2 transformer model. Return that text as a String. Load the model from modelpath and the corresponding tokenizer from vocabpath (which defaults to \"data/tokenizer.bin\"). Take an initial prompt String to start the text generation and generate up to max_tokens tokens. If verbose, print the text during generation. Set temperature and topp to configure the Sampler.\n\nArguments\n\nmodelpath::String: Path to the binary model file (e.g., \"stories15M.bin\").\nprompt::String: Initial text to condition generation (default: empty, starts with BOS token).\n\nKeyword Arguments\n\nmax_tokens::Int=255: Maximum number of tokens to generate.\nvocabpath::String: Path to tokenizer binary (default: \"data/tokenizer.bin\").\nverbose::Bool=false: If true, print tokens as they are generated.\ntemperature::Float32=0.0f0: Sampling temperature (0 = greedy, higher = more random).\ntopp::Float32=1.1f0: Nucleus sampling threshold (≤0 or ≥1 disables nucleus sampling).\n\nReturns\n\nString: The generated text.\n\njulia> print(talktollm(\"/PATH/TO/YOUR/MODEL.bin\"))\n Once upon a time, there was a little girl named Lily. She loved to play outside in the park with her friends. One day, Lily was running and she fell and hit her head on a rock. She got a big ouchie and it started to bleed. \nLily's mom took her to the doctor and the doctor said she needed a stitch. Lily was scared, but her mom was very dependable and told her they would be coming back home soon. \nAfter the doctor fixed Lily's knee, they went home and Lily's friends came to play again. But Lily's mom noticed that she was playing with a ball and some new toys. This made her very happy.\n\njulia> print(talktollm(\"/PATH/TO/YOUR/MODEL.bin\", \"\"What is this?\"\"))\n\"What is this?\" the woman asked.\nThe little girl looked at the bookion and said, \"This is a book about a princess. Maybe we can use it together.\"\nThey decided to sit down and read the book together. They read about a beautiful garden with lovely flowers. The little girl loved the book very much and said, \"I want to be a princess again!\"\n\"Maybe, if you read me another book,\" the woman said.\nFrom that day on, they would sit down and read the book every night before bed. They hoped that when they finished reading it, something magical would happen.\n\n\n\n\n\n","category":"function"},{"location":"inference/#Llama2.ChatBot","page":"Inference","title":"Llama2.ChatBot","text":"ChatBot(path::String; vocabpath::String)\n\nCreate a ChatBot constructing a Transformer from path.\n\nvocabpath defaults to \"data/tokenizer.bin\". The ChatBot struct is used with chatwithllm for continuous text generation.\n\n\n\n\n\n","category":"type"},{"location":"inference/#Llama2.chatwithllm","page":"Inference","title":"Llama2.chatwithllm","text":"chatwithllm(bot::ChatBot, [prompt::String]; max_tokens::Int, verbose::Bool)\n\nGenerate text using a pretrained LLama2 transformer model. Return that text as a String.\n\nMultiple calls on the same instance of ChatBot respect the previously generated tokens and continue generation from there. Take an initial prompt String to start the text generation and generate up to max_tokens tokens. If verbose, print the text during generation.\n\njulia> c = ChatBot(\"data/stories15M.bin\");\n\njulia> print(chatwithllm(c); max_tokens = 63)\n Once upon a time, there was an old house with an ancient sign inside. The sign was very big and had many words on it. One day, a little girl went to visit the old house. She wanted to see what was inside.\nThe old house said, \"Hello? Can I come in?\"\n\njulia> print(chatwithllm(c, \"\nThe little girl said:\"; max_tokens = 63))\n\nThe little girl said: \"Yes please! Can I come in too?\"\nThe old house thought for moments before it said, \"Yes. This light is available for you 30 cent a nightmare.\"\nThe little girl was very excited. She said thank you and then, followed her favorite sign\n\njulia> print(chatwithllm(c, \"until\"; max_tokens = 63))\nuntil she saw there was a beautiful light online.\nWhen the old house passed, the girl happily went inside. It was very old, but it had been there for a long time. The old house was very special, and she thought the light was the prettiest thing ever.\n\n\n\n\n\n","category":"function"}]
}
